{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swastikbanerjee/NLP_Lab/blob/main/nlpLab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idEWcC5VR2lx",
        "outputId": "85faf94e-580f-4fa7-e5f9-2c47248edc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'do\", \"n't\", \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', '.', 'And', ',', 'let', \"'s\", 'not', 'forget', 'about', 'emojis', '!', 'üòäüëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', '.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄüíª']\n",
            "Sentence Tokenization: [\" Negation, or the use of words like 'don't' to indicate the opposite meaning of a sentence, can also be tricky for NLP algorithms to handle.\", \"And, let's not forget about emojis!\", 'üòäüëç Adding more than one emoji in a sentence can make it even harder for NLP algorithms to understand the intended meaning.', 'Despite these challenges, NLP is a rapidly growing field with a lot of exciting potential applications!', 'üöÄüíª']\n",
            "Punctuation-based Tokenizer: ['Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'\", 'don', \"'\", 't', \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', '.', 'And', ',', 'let', \"'\", 's', 'not', 'forget', 'about', 'emojis', '!', 'üòäüëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', '.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄüíª']\n",
            "Treebank Word tokenizer: ['Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'do\", \"n't\", \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle.', 'And', ',', 'let', \"'s\", 'not', 'forget', 'about', 'emojis', '!', 'üòäüëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄüíª']\n",
            "Tweet Tokenizer: ['Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'\", \"don't\", \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', '.', 'And', ',', \"let's\", 'not', 'forget', 'about', 'emojis', '!', 'üòä', 'üëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', '.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄ', 'üíª']\n",
            "Multi-Word Expression Tokenizer: ['Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'do\", \"n't\", \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', '.', 'And', ',', 'let', \"'s\", 'not', 'forget', 'about', 'emojis', '!', 'üòäüëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', '.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄüíª']\n",
            "TextBlob Word Tokenize: ['Negation', 'or', 'the', 'use', 'of', 'words', 'like', \"'do\", \"n't\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', 'And', 'let', \"'s\", 'not', 'forget', 'about', 'emojis', 'üòäüëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', 'Despite', 'these', 'challenges', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', 'üöÄüíª']\n",
            "spaCy Tokenizer: [' ', 'Negation', ',', 'or', 'the', 'use', 'of', 'words', 'like', \"'\", 'do', \"n't\", \"'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', ',', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', '.', 'And', ',', 'let', \"'s\", 'not', 'forget', 'about', 'emojis', '!', 'üòä', 'üëç', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', '.', 'Despite', 'these', 'challenges', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', '!', 'üöÄ', 'üíª']\n",
            "Gensim word tokenizer: ['Negation', 'or', 'the', 'use', 'of', 'words', 'like', 'don', 't', 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', 'can', 'also', 'be', 'tricky', 'for', 'NLP', 'algorithms', 'to', 'handle', 'And', 'let', 's', 'not', 'forget', 'about', 'emojis', 'Adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'NLP', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', 'Despite', 'these', 'challenges', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications']\n",
            "Tokenization with Keras: ['negation', 'or', 'the', 'use', 'of', 'words', 'like', \"'don't'\", 'to', 'indicate', 'the', 'opposite', 'meaning', 'of', 'a', 'sentence', 'can', 'also', 'be', 'tricky', 'for', 'nlp', 'algorithms', 'to', 'handle', 'and', \"let's\", 'not', 'forget', 'about', 'emojis', 'üòäüëç', 'adding', 'more', 'than', 'one', 'emoji', 'in', 'a', 'sentence', 'can', 'make', 'it', 'even', 'harder', 'for', 'nlp', 'algorithms', 'to', 'understand', 'the', 'intended', 'meaning', 'despite', 'these', 'challenges', 'nlp', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'a', 'lot', 'of', 'exciting', 'potential', 'applications', 'üöÄüíª']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, MWETokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \" Negation, or the use of words like 'don't' to indicate the opposite meaning of a sentence, can also be tricky for NLP algorithms to handle. And, let's not forget about emojis! üòäüëç Adding more than one emoji in a sentence can make it even harder! for NLP algorithms to understand the intended meaning. Despite these challenges, NLP is a rapidly growing field with a lot of exciting potential applications! üöÄüíª\"\n",
        "# a.Word Tokenization\n",
        "nltk.download('punkt')\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", word_tokens)\n",
        "\n",
        "# b.Sentence Tokenization\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sent_tokens)\n",
        "\n",
        "# c.Punctuation-based Tokenizer\n",
        "punctuation_tokens = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
        "print(\"Punctuation-based Tokenizer:\", punctuation_tokens)\n",
        "\n",
        "# d.Treebank Word tokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer().tokenize(text)\n",
        "print(\"Treebank Word tokenizer:\", treebank_tokenizer)\n",
        "\n",
        "# e.Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer().tokenize(text)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokenizer)\n",
        "\n",
        "# f.Multi-Word Expression Tokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Word', 'Tokenization'), ('Tokenization', 'Python')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
        "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
        "\n",
        "# g.TextBlob Word Tokenize\n",
        "textblob_tokens = TextBlob(text).words\n",
        "print(\"TextBlob Word Tokenize:\", textblob_tokens)\n",
        "\n",
        "# h.spaCy Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
        "\n",
        "# i.Gensim word tokenizer\n",
        "gensim_tokens = list(tokenize(text))\n",
        "print(\"Gensim word tokenizer:\", gensim_tokens)\n",
        "\n",
        "# j.Tokenization with Keras\n",
        "keras_tokens = text_to_word_sequence(text)\n",
        "print(\"Tokenization with Keras:\", keras_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. **Word Tokenization**:\n",
        "   - Definition: Word tokenization is the process of splitting a text into individual words or to::kens based on whitespace or punctuation.:\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "b. **Sentence Tokenization**:\n",
        "   - Definition: Sentence tokenization is the process of splitting a text into individual sentences.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you? I'm doing great.\"\n",
        "     Output: [\"Hello, world!\", \"How are you?\", \"I'm doing great.\"]\n",
        "\n",
        "c. **Punctuation-based Tokenizer**:\n",
        "   - Definition: Punctuation-based tokenizer splits a text into tokens based on punctuation marks such as commas, periods, etc.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "d. **Treebank Word Tokenizer**:\n",
        "   - Definition: Treebank word tokenizer is a word tokenizer that follows the conventions used in the Penn Treebank corpus.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "e. **Tweet Tokenizer**:\n",
        "   - Definition: Tweet tokenizer is specialized for tokenizing tweets, considering emojis, hashtags, and handles.\n",
        "   - Example:\n",
        "     Input: \"Just landed ‚úàÔ∏è in New York! #excited\"\n",
        "     Output: ['Just', 'landed', '‚úàÔ∏è', 'in', 'New', 'York', '!', '#excited']\n",
        "\n",
        "f. **Multi-Word Expression Tokenizer**:\n",
        "   - Definition: Multi-Word Expression tokenizer recognizes specific multi-word expressions and treats them as single tokens.\n",
        "   - Example:\n",
        "     Input: \"New York City is a great place to visit.\"\n",
        "     Output: ['New York City', 'is', 'a', 'great', 'place', 'to', 'visit']\n",
        "\n",
        "g. **TextBlob Word Tokenize**:\n",
        "   - Definition: TextBlob word tokenize is used to tokenize text into individual words using the TextBlob library.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', 'world', 'How', 'are', 'you']\n",
        "\n",
        "h. **spaCy Tokenizer**:\n",
        "   - Definition: spaCy tokenizer tokenizes the text using the spaCy library, providing detailed tokenization including parts of speech tagging and entity recognition.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "i. **Gensim word tokenizer**:\n",
        "   - Definition: Gensim word tokenizer is a simple tokenizer provided by the Gensim library.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', 'world', 'How', 'are', 'you']\n",
        "\n",
        "j. **Tokenization with Keras**:\n",
        "   - Definition: Tokenization with Keras is similar to word tokenization but converts the text to lowercase and removes punctuation.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['hello', 'world', 'how', 'are', 'you']"
      ],
      "metadata": {
        "id": "CldheXonUsPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Word Tokenization**:\n",
        "   - Insight: Word tokenization is a fundamental step in natural language processing (NLP) tasks, breaking down text into its constituent words.\n",
        "   - Applications: Sentiment analysis, text classification, language translation, named entity recognition.\n",
        "\n",
        "2. **Sentence Tokenization**:\n",
        "   - Insight: Sentence tokenization divides text into individual sentences, enabling analysis at a higher level of granularity.\n",
        "   - Applications: Text summarization, machine translation, text-to-speech systems, document indexing.\n",
        "\n",
        "3. **Punctuation-based Tokenizer**:\n",
        "   - Insight: Punctuation-based tokenizer segments text based on punctuation marks, preserving them as separate tokens.\n",
        "   - Applications: Sentiment analysis, parsing URLs or email addresses, analyzing text with emoticons or special characters.\n",
        "\n",
        "4. **Treebank Word Tokenizer**:\n",
        "   - Insight: Treebank word tokenizer follows conventions used in the Penn Treebank corpus, providing standardized tokenization.\n",
        "   - Applications: Part-of-speech tagging, dependency parsing, named entity recognition, syntactic analysis.\n",
        "\n",
        "5. **Tweet Tokenizer**:\n",
        "   - Insight: Tweet tokenizer handles tokenization in the context of social media text, which often contains hashtags, mentions, and emojis.\n",
        "   - Applications: Social media sentiment analysis, trend analysis, user profiling, brand monitoring.\n",
        "\n",
        "6. **Multi-Word Expression Tokenizer**:\n",
        "   - Insight: Multi-word expression tokenizer recognizes and treats multi-word phrases as single tokens, preserving their semantic integrity.\n",
        "   - Applications: Named entity recognition, extracting domain-specific terminology, phrase-based machine translation.\n",
        "\n",
        "7. **TextBlob Word Tokenize**:\n",
        "   - Insight: TextBlob word tokenize provides a simple and easy-to-use tokenization method integrated with other NLP functionalities.\n",
        "   - Applications: Basic NLP tasks like sentiment analysis, part-of-speech tagging, text classification.\n",
        "\n",
        "8. **spaCy Tokenizer**:\n",
        "   - Insight: spaCy tokenizer offers detailed tokenization with linguistic features such as part-of-speech tagging and named entity recognition.\n",
        "   - Applications: Advanced NLP tasks including entity recognition, information extraction, syntactic parsing.\n",
        "\n",
        "9. **Gensim word tokenizer**:\n",
        "   - Insight: Gensim word tokenizer is a lightweight tool for basic tokenization tasks, particularly suited for large-scale text processing.\n",
        "   - Applications: Topic modeling, document clustering, word embedding generation, semantic similarity calculation.\n",
        "\n",
        "10. **Tokenization with Keras**:\n",
        "    - Insight: Tokenization with Keras is tailored for text preprocessing in deep learning models, converting text to lowercase and removing punctuation.\n",
        "    - Applications: Text classification, sentiment analysis, sequence-to-sequence tasks, neural machine translation."
      ],
      "metadata": {
        "id": "H9Kuag5ZVtQ6"
      }
    }
  ]
}